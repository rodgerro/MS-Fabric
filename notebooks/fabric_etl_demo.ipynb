{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fabric ETL Demo (PySpark)\n",
        "This notebook ingests CSVs from **/Files/data** in a Fabric Lakehouse, cleans them, and writes curated Delta Tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Fabric/Databricks-compatible PySpark\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "# Adjust base path if needed. In Fabric, upload CSVs to Lakehouse Files: /Files/data\n",
        "base_path = \"/lakehouse/default/Files/data\"  # In Fabric, mount differs by environment; change as needed.\n",
        "customers_path = f\"{base_path}/customers.csv\"\n",
        "orders_path = f\"{base_path}/orders.csv\"\n",
        "\n",
        "df_customers = (spark.read\n",
        "    .option(\"header\", True)\n",
        "    .csv(customers_path)\n",
        ")\n",
        "\n",
        "df_orders = (spark.read\n",
        "    .option(\"header\", True)\n",
        "    .csv(orders_path)\n",
        "    .withColumn(\"Qty\", F.col(\"Qty\").cast(\"int\"))\n",
        "    .withColumn(\"UnitPrice\", F.col(\"UnitPrice\").cast(\"double\"))\n",
        "    .withColumn(\"OrderDate\", F.to_date(\"OrderDate\"))\n",
        "    .withColumn(\"LineTotal\", F.col(\"Qty\") * F.col(\"UnitPrice\"))\n",
        ")\n",
        "\n",
        "df_customers.createOrReplaceTempView(\"stg_customers\")\n",
        "df_orders.createOrReplaceTempView(\"stg_orders\")\n",
        "display(df_orders.limit(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Join and curate a simple star-like table\n",
        "df_fact = spark.sql('''\n",
        "SELECT o.OrderID,\n",
        "       o.OrderDate,\n",
        "       o.CustomerID,\n",
        "       c.FirstName,\n",
        "       c.LastName,\n",
        "       c.City,\n",
        "       c.State,\n",
        "       o.SKU,\n",
        "       o.Qty,\n",
        "       o.UnitPrice,\n",
        "       o.LineTotal\n",
        "FROM stg_orders o\n",
        "LEFT JOIN stg_customers c\n",
        "  ON o.CustomerID = c.CustomerID\n",
        "''')\n",
        "\n",
        "display(df_fact.limit(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Write to Lakehouse Tables (Delta)\n",
        "catalog_table = \"fact_orders\"\n",
        "df_fact.write.mode(\"overwrite\").saveAsTable(catalog_table)\n",
        "print(f\"Wrote table: {catalog_table}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "- Open the **SQL analytics endpoint** for your Lakehouse and query `fact_orders`.\n",
        "- Connect a **Power BI semantic model** via **Direct Lake**.\n",
        "- Add measures like `Total Sales = SUM(fact_orders[LineTotal])`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}